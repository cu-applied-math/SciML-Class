{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5pjf9MZi0Cs0s5vGVeeP6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cu-applied-math/SciML-Class/blob/main/Labs/lab05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 5 for SciML\n",
        "Why do we hvae validation and testing sets?"
      ],
      "metadata": {
        "id": "HVxFoz_1eIll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XJVt6Lu1L8E"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-sided [Hoeffding inequality](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality#Special_Case:_Bernoulli_RVs) for Bernoulli r.v.\n",
        "$$\n",
        "\\mathbb{P}\\left[ \\left| \\frac{1}{n}S_n - \\mu  \\right| \\ge t\\right] \\le 2 \\exp\\left( -2nt^2\\right)\n",
        "$$\n",
        "where $S_n=Z_1+Z_2+\\ldots+Z_n$ is the sum of $n$ **independent** Bernoulli random variables.\n",
        "\n",
        "## Part 1\n",
        "Let $\\mu$ be the true risk of a binary classifier $f$ (using the 0-1 loss $\\ell$), and suppose we have $n$ independent samples (that are also independent of $f$), and let $\\frac{1}{n}S_n$ be the empirical risk (still using the 0-1 loss) on these samples.  (Think of these as validation or testing samples)\n",
        "\n",
        "**Q**: If we want to estimate $\\mu$ to an accuracy of $\\pm t$, with a confidence of $p=1-\\alpha$, how many samples $n$ do we need? Do this for $t=0.001, 0.01, 0.02$ and $0.05$, and for $\\alpha$ being $10^{-1},10^{-2},10^{-3},10^{-4},10^{-5}$. Make a table of the resulting $n$ values.\n",
        "\n",
        "Comment on whether $p$ or $t$ has a \"bigger effect\".\n",
        "\n",
        "**A**:"
      ],
      "metadata": {
        "id": "vaAhwx6u11ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sep=\"\"  # for printing to screen\n",
        "# sep=\"|\" # for copying to markdown\n",
        "\n",
        "\n",
        "pList = np.logspace(-5,-1,5)\n",
        "print(end=sep)\n",
        "print(\"        \",end=sep)\n",
        "for p in pList:\n",
        "    print(f\"{p:10.0e}\",end=sep)\n",
        "    n = np.log(2/p)/(2*0.001**2)\n",
        "print(flush=True)\n",
        "for t in [0.001, 0.01, 0.02, 0.05]:\n",
        "    nValues = []\n",
        "    print(end=sep)\n",
        "    print(f\"{t:.3f}\",end=\"\\t\")\n",
        "    print(end=sep)\n",
        "    for p in pList:\n",
        "\n",
        "        n = ???????\n",
        "\n",
        "        print(f\"{int(n):-10d}\",end=sep)\n",
        "    print(flush=True)"
      ],
      "metadata": {
        "id": "y_nLztxz10ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility code (for parts 2--4)"
      ],
      "metadata": {
        "id": "y6DVCjMCHJq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "9UxIKfMY4uKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(101)\n",
        "d = 1  # dimension\n",
        "\n",
        "def get_feature_samples(n):\n",
        "    return torch.rand( (n,d) )-0.5 # uniform in [-.5,5]^d\n",
        "\n",
        "def f(x):\n",
        "    \"\"\" the *true* function that we're trying to learn\n",
        "    Should be 0 and 1 outputs\n",
        "    \"\"\"\n",
        "    y = (torch.sign( torch.sin( 15*x + 50*x**2 ) ) + 1 )/2\n",
        "    return y.long() # the datatype torch expects for classification\n",
        "\n",
        "# if d == 1:\n",
        "#     # Easy to plot\n",
        "#     X_plot = torch.linspace(-.5,.5,300)\n",
        "#     y_plot = f(X_plot)\n",
        "#     plt.plot( X_plot, y_plot )\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "n_test      = int(1e5) # Pick enough to make this the \"truth\"\n",
        "X_test      = get_feature_samples(n_test)\n",
        "y_test      = f(X_test) # can use torch.vmap if needed\n",
        "testset     = torch.utils.data.TensorDataset(X_test,  y_test)\n",
        "testloader  = torch.utils.data.DataLoader(testset,batch_size=1000)\n",
        "\n",
        "vals,bins   = torch.histogram(y_test.float(),bins=2)\n",
        "print(vals/n_test) # check if the data is approximately well-balanced\n",
        "# print(bins)\n",
        "\n",
        "\n",
        "class Net( nn.Module ):\n",
        "    \"\"\" A simple fully-connected neural net, all hidden layers the same size\n",
        "    This does binary classification by default (outputDim=2), setup for cross-entropy loss\n",
        "    \"\"\"\n",
        "    def __init__(self, nHiddenLayers=5, width=100, activation = nn.ReLU(), outputDim=2, inputDim = 1):\n",
        "        super().__init__()\n",
        "        # Construct a net as instructed (all hidden layers of the same size)\n",
        "        self.net = []\n",
        "        self.net.append(nn.Linear(inputDim, width))\n",
        "        self.net.append( activation )\n",
        "        for i in range(nHiddenLayers):\n",
        "            self.net.append(nn.Linear(width, width))\n",
        "            self.net.append( activation )\n",
        "        self.net.append(nn.Linear(width, outputDim))\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def empiricalRisk( model, dataloader ):\n",
        "    \"\"\" We need a dataloader, not a dataset\n",
        "    (otherwise the size of the tensor isn't correct)\n",
        "    Assumes the true labels are [0,1,...,outputdim-1]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total   = 0\n",
        "        correct = 0\n",
        "        for data in dataloader:\n",
        "            inputs, trueLabels = data\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs.data, 1)\n",
        "            total   += trueLabels.size(0)\n",
        "            # Careful: if predicted is of size (batchSize,1)\n",
        "            #   and trueLabels are of size (batchSize,) then using \"==\"\n",
        "            #   will implicitly broadcast them to the wrong size!\n",
        "            correct += (predicted.ravel() == trueLabels.ravel() ).sum().item()\n",
        "            # We did .sum() in case batch size > 1\n",
        "    return correct/total\n",
        "\n",
        "def train_model( trainloader, optimizer, epochs = 30, criterion = nn.CrossEntropyLoss()  ):\n",
        "    \"\"\"\n",
        "    You don't explicitly pass in the model, but that's implicit in the optimizer\n",
        "    \"\"\"\n",
        "    for t in range(epochs):\n",
        "        runningLoss = 0.\n",
        "        for data in trainloader:\n",
        "            inputs, trueLabels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss    = criterion(outputs,trueLabels.flatten() )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            runningLoss += loss.item()"
      ],
      "metadata": {
        "id": "OM-93qkpHc1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: for a fixed model, look at validation risk\n",
        "\n",
        "Fix some classifier $h$ (i.e., model), and for each data point $(X_i,y_i)$ for $i=1,\\ldots,n_\\text{validation}$, we set the random variable\n",
        "$$\n",
        "Z_i = \\begin{cases} 1 & y_i = h(X_i) \\\\ 0 & y_i \\neq h(X_i)\\end{cases}\n",
        "$$\n",
        "so $S_n$ represents the **accuracy** (i.e., empirical risk, if we use the 0-1 loss) of our model $h$ on this data.\n",
        "\n",
        "We'll find $\\mu$ by evaluating accuracy on a huge number of testing points.\n",
        "\n",
        "Then, for many **experiments**, draw a new validation set (of size $n_\\text{validation}=1000$) and we compare that estimate $S_n$ with $\\mu$.  We do many experiments so that we can estimate the probability of $|S_n - \\mu| > t$  (we can choose $t$ *after* we run the experiments)\n"
      ],
      "metadata": {
        "id": "JVH4uKHbCp8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(nHiddenLayers=3, width=20 )\n",
        "trueRisk    = empiricalRisk(model, testloader)\n",
        "print(f\"True risk: {trueRisk}\")\n",
        "\n",
        "# Use many validation sets\n",
        "n_experiments = 1000\n",
        "for i in range(n_experiments):\n",
        "    X_valid     = get_feature_samples(n_valid)\n",
        "    y_valid     = f(X_valid)\n",
        "    validset    = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
        "    validloader = torch.utils.data.DataLoader(validset,batch_size=1000)\n",
        "\n",
        "    .... do something, record something, ...\n",
        "\n",
        "\n",
        "... do some analysis... compare with Hoeffding bound"
      ],
      "metadata": {
        "id": "BEUk1-rZKgpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: choose the best from among many models\n",
        "Now, fix a given validation set, but we'll do many experiments with sets of models.\n",
        "\n",
        "That is, for a set of 10 models, we'll choose the best model (in terms of accuracy; higher is better)\n"
      ],
      "metadata": {
        "id": "nJ54ZcBpVrZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a single validation set...\n",
        "X_valid     = get_feature_samples(n_valid)\n",
        "y_valid     = f(X_valid)\n",
        "validset    = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
        "validloader = torch.utils.data.DataLoader(validset,batch_size=1000)\n",
        "\n",
        "...."
      ],
      "metadata": {
        "id": "l032hDLCJUca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: training and choosing best hyperparameters\n",
        "\n",
        "Same as part 3, but now we'll optimize the models some"
      ],
      "metadata": {
        "id": "OkExQPoFTZX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_train     = int(1e3) # not too large, in order to keep this fast\n",
        "X_train     = get_feature_samples(n_train)\n",
        "y_train     = f(X_train)\n",
        "trainset    = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)\n",
        "criterion   = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use a single validation set...\n",
        "X_valid     = get_feature_samples(n_valid)\n",
        "y_valid     = f(X_valid)\n",
        "validset    = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
        "validloader = torch.utils.data.DataLoader(validset,batch_size=1000)\n",
        "\n",
        "# Hyperparameters:\n",
        "nHiddenLayers = 3\n",
        "width         = 20\n",
        "learning_rate = 0.01\n",
        "epochs        = 1  # let's make it fast!\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "    model = Net(nHiddenLayers=nHiddenLayers, width=width )\n",
        "    model.train()\n",
        "    optimizer     = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "    # Train it\n",
        "    train_model( trainloader, optimizer, epochs=epochs )\n",
        "\n",
        "\n",
        "...\n"
      ],
      "metadata": {
        "id": "ok1LqF5_MYpX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}