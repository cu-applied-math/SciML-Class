{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNHFdSbrGmNdIJsGGvB8+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cu-applied-math/SciML-Class/blob/main/Labs/lab10_curse_of_dimensionality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8V3iSILXk3m6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.integrate import quad, nquad, simpson\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 10: curse of dimensionality and integration\n",
        "\n",
        "The \"curse of dimensionality\" is a term thrown around a lot these days, and it means different things to different people and in different contexts.  One context in which it is used is when the work required to reach a fixed accuracy grows exponentially in the dimension.\n",
        "\n",
        "We'll explore this phenomenon in the context of integration, and compare two methods (quadrature vs Monte Carlo) which have very different pros and cons.\n",
        "\n",
        "- Bonus: if you're interested in a method that mixes the pros and cons of each approach, look into **quasi Monte Carlo** methods. I have a [demo on quasi-Monte Carlo](https://github.com/stephenbeckr/randomized-algorithm-class/blob/master/Demos/demo14_MonteCarlo_and_improvements.ipynb) that I made for a different class"
      ],
      "metadata": {
        "id": "w3TGFW-dDVK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: quadrature in 1D\n",
        "\n",
        "\"Quadrature\" refers to numerical integration where we sample points from the domain and use specific weights associated with those points to form our estimate of the integral (in contrast, Monte Carlo and Quasi-Monte Carlo are sometimes defined to be estimates in which the weights never change from point to point, though that definition is too vague to be illuminating at this point).\n",
        "\n",
        "For example, the midpoint rule, [trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule), and [Simpson's rule](https://en.wikipedia.org/wiki/Simpson%27s_rule) (and their composite version) are quadrature methods, as are things like [Gauss-Legendre](https://en.wikipedia.org/wiki/Gaussian_quadrature) and [Clenshaw-Curtis](https://en.wikipedia.org/wiki/Clenshaw%E2%80%93Curtis_quadrature).\n",
        "\n",
        "We won't investigate these in detail, as we'll use `scipy.integrate.quad` to do the work for us (and it adaptively chooses the number of nodes in order to reach a target accuracy), but it helps to think of these as just Riemann sums.\n",
        "\n",
        "Throughout this lab, we'll work with the function $f(x)=\\sin(x)$, or in higher dimensions with $\\vec{x}\\in\\mathbb{R}^d$, then\n",
        "$$f(\\vec{x}) = \\prod_{i=1}^d \\sin(x_i)$$\n",
        "and our goal is to estimate its integral\n",
        "$$\\widetilde{I} = \\int_{a}^{b} f(x)dx$$\n",
        "or more generally if $d\\ge 1$\n",
        "$$\\widetilde{I} = \\int_{a}^{b}\\int_{a}^{b} \\ldots \\int_{a}^{b} f(\\vec{x})dx_1 dx_2 \\ldots dx_d.$$\n",
        "We chose $f$ to be a simple function so that you can workout the answer by hand.\n",
        "\n",
        "When $d>1$, let's actually look at\n",
        "$$I = \\widetilde{I}^{1/d}$$\n",
        "since the value of this will not depend on the dimension."
      ],
      "metadata": {
        "id": "NF4Bz2gBSM6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1a: determine the value of $I$ by hand for $a=0$ and $b=\\pi$\n",
        "... so that you can check your answer.\n",
        "\n",
        "Do this for any **arbitrary** dimension $d\\in\\mathbb{N}$.\n",
        "\n",
        "And in fact if $b=k \\pi$ for any integer $k$, you can easily work out the true answer also."
      ],
      "metadata": {
        "id": "G9KUJACHUEnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trueIntegral = ... TODO ...\n",
        "\n",
        "# You can either hardcode the value for [0,pi]\n",
        "# or make a function that takes in any [a,b]. Either way is fine"
      ],
      "metadata": {
        "id": "-FjVaDX1UKsN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1b: accuracy vs number of points\n",
        "For $a=0$ and $b=\\pi$, compute the quadrature with a varying number of points, and **plot** the error vs the number of points.\n",
        "\n",
        "Do this for 1d only (for now)\n",
        "\n",
        "Do this using Simpson's rule (`scipy.integrate.simpson`), giving it a fixed grid of nodes (e.g., `x=np.linspace(...)`) so that you can control the accuracy (if you use `scipy.integrate.quad`, it's a bit harder to force it to give you low accuracy)\n",
        "\n",
        "- **Bonus** question: if we choose $b=2\\pi$ (and adjust our true answer accordingly), how does the error decay with the number of points? Did you expect this?  (good undergrad numerics books should discuss this; the phenomenon is **spectral accuracy**; see, e.g., [this paper](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=29d832b1235e61ad5445bceb90ecf8a2bf857044) which has links to some textbooks in the references)."
      ],
      "metadata": {
        "id": "FM0z5Mp9UMAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return np.sin(x)\n",
        "a = 0\n",
        "b = np.pi\n",
        "\n",
        "nNodes = 100 # for example...\n",
        "\n",
        "\n",
        "x = np.linspace(a, b, nNodes)\n",
        "integral = simpson(f(x),x=x) # this is how to use `simpson`\n",
        "print(integral)\n",
        "\n",
        "# Now do this for a range of nNodes values, save the output, and plot it\n",
        "# ..."
      ],
      "metadata": {
        "id": "4ZiQFxwiYuOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1c: quadrature error as a function of dimension\n",
        "Now we can use `scipy.integrate.quad` (for 1D) or `scipy.integrate.nquad` (for arbitrary dimensions).\n",
        "\n",
        "#### (i) Run `scipy.integrate.quad` in 1D (still for $a=0, b=\\pi$) and check your answer to confirm that you're using it correctly.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GGHfEjqRcIm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dim  = 1\n",
        "a, b = 0, np.pi\n",
        "\n",
        "ranges = [[a,b]]\n",
        "def f(x):\n",
        "    return np.sin(x)\n",
        "\n",
        "stuff = nquad( ... )\n",
        "# If you want to get more information back, use nquad( ... , full_output=True )\n",
        "\n",
        "stuff # stuff[0] is the value, stuff[1] is the error estimate"
      ],
      "metadata": {
        "id": "MKqBj67vHplz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (ii) Then run `scipy.integrate.nquad` in 2D (now for the square $[0,\\pi]^2$) and check your answer to confirm that you're using it correctly.\n"
      ],
      "metadata": {
        "id": "f-xc_3sJgbob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ... TODO ..."
      ],
      "metadata": {
        "id": "9t9K8qhoI2gk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (iii) Then make a plot for # of function evaluations as a function of dimension, for dimensions $d=1,2,3,\\ldots$ (as large as you can go)"
      ],
      "metadata": {
        "id": "FF_El4_Ygj6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ... TODO ..."
      ],
      "metadata": {
        "id": "TcZBNUczI99Q"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Monte Carlo\n",
        "\n",
        "We think of $\\int_{a}^b f(x)dx$ as an expected value, like\n",
        "$$E[\\tilde{f}] := \\int_{a}^b \\tilde{f}(x) p(x) dx$$\n",
        "for a probability distribution $p$ (i.e., $\\int_a^b p(x)dx = 1$ and $p(x)\\ge 0$), with $\\tilde{f}(x) := f(x)/p(x)$.\n",
        "\n",
        "The simplest choice is the uniform distribution for $p$, so\n",
        "$$p(x) = \\frac{1}{b-a}.$$\n",
        "\n",
        "Then to estimate the value of the integral, we draw random samples $x$ from this uniform distribution $p$, and then evaluate $\\tilde{f}(x)$. We do this many times, to get a **sample mean**, and use that to approximate the true mean.\n",
        "\n",
        "That's it!"
      ],
      "metadata": {
        "id": "1hrA2AigMJjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2a: Monte Carlo in 1D\n",
        "Plot error as a function of number of sampled points"
      ],
      "metadata": {
        "id": "O2JEbS1arFLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dim = 1\n",
        "nPoints = int(1e6)\n",
        "b = np.pi\n",
        "a = 0\n",
        "\n",
        "def f(X):\n",
        "    \"\"\" sin(x) where x may be a matrix;\n",
        "    in that case, it returns one output per row.\n",
        "    For a given row, this does the product of sin(x_i) for each column i\n",
        "    \"\"\"\n",
        "    return np.sin(X).prod(axis=1)\n",
        "\n",
        "# Efficient way to get f(x) for a range of x values:\n",
        "X = np.random.uniform(a,b, (nPoints, dim))\n",
        "fX = f(X)\n",
        "\n",
        "# Now, do something with all these values...\n",
        "# ... TODO ..."
      ],
      "metadata": {
        "id": "WHvsLuIRMKc-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2b: Monte Carlo in high dimensions\n",
        "\n",
        "For a range of dimensions, look at the error of Monte Carlo using 1e4 points. The answer will be random, so let's do this 100 times and average."
      ],
      "metadata": {
        "id": "DY7UJusNrKhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(X):\n",
        "    \"\"\" sin(x) where x may be a matrix;\n",
        "    in that case, it returns one output per row.\n",
        "    For a given row, this does the product of sin(x_i) for each column i\n",
        "    \"\"\"\n",
        "    return np.sin(X).prod(axis=1)\n",
        "\n",
        "nPoints = int(1e4)\n",
        "b = np.pi\n",
        "a = 0\n",
        "\n",
        "# ... TODO ..."
      ],
      "metadata": {
        "id": "_yN5R1XXNr0r"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: writeup\n",
        "To get credit for the lab,\n",
        "- write a sentence or two about your observations, and...\n",
        "- from part 1b, using about 100 function evalutions, what was your accuracy?\n",
        "- from part 1c, in dimension 4, how many function evaluations did you use?\n",
        "- from part 2, in dimension 1, with 1000 function evaluations, what was the accuracy?'"
      ],
      "metadata": {
        "id": "g-6vFa35cwWo"
      }
    }
  ]
}